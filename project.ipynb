{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  label\n",
      "0      dear american teens question dutch person hear...      0\n",
      "1      nothing look forward lifei dont many reasons k...      1\n",
      "2      music recommendations im looking expand playli...      0\n",
      "3      im done trying feel betterthe reason im still ...      1\n",
      "4      worried  year old girl subject domestic physic...      1\n",
      "...                                                  ...    ...\n",
      "27972  posting everyday people stop caring  religion ...      0\n",
      "27973  okay definetly need hear guys opinion ive pret...      0\n",
      "27974  cant get dog think ill kill myselfthe last thi...      1\n",
      "27975  whats point princess bridei really think like ...      1\n",
      "27976  got nudes person might might know snapchat do ...      0\n",
      "\n",
      "[27977 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load data from CSV file\n",
    "data = pd.read_csv('mental_health.csv')\n",
    "\n",
    "# Display first few rows to understand the structure of the data\n",
    "print(data)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data['text'].astype(str)\n",
    "y = data['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Fit and transform on training data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform test data using the same vectorizer\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Determine input_dim (number of features)\n",
    "input_dim = X_train.shape[1]  # This will be 5000 in this example\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               640128    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 648,449\n",
      "Trainable params: 648,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer and hidden layers\n",
    "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dropout(0.2))  # Dropout for regularization\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "700/700 [==============================] - 6s 7ms/step - loss: 0.2449 - accuracy: 0.8998 - val_loss: 0.1915 - val_accuracy: 0.9242\n",
      "Epoch 2/10\n",
      "700/700 [==============================] - 5s 8ms/step - loss: 0.1464 - accuracy: 0.9425 - val_loss: 0.2032 - val_accuracy: 0.9208\n",
      "Epoch 3/10\n",
      "700/700 [==============================] - 6s 8ms/step - loss: 0.1014 - accuracy: 0.9637 - val_loss: 0.2314 - val_accuracy: 0.9176\n",
      "Epoch 4/10\n",
      "700/700 [==============================] - 6s 8ms/step - loss: 0.0578 - accuracy: 0.9805 - val_loss: 0.2954 - val_accuracy: 0.9155\n",
      "Epoch 5/10\n",
      "700/700 [==============================] - 5s 7ms/step - loss: 0.0269 - accuracy: 0.9916 - val_loss: 0.3587 - val_accuracy: 0.9153\n",
      "Epoch 6/10\n",
      "700/700 [==============================] - 5s 8ms/step - loss: 0.0126 - accuracy: 0.9958 - val_loss: 0.4409 - val_accuracy: 0.9142\n",
      "Epoch 7/10\n",
      "700/700 [==============================] - 5s 8ms/step - loss: 0.0075 - accuracy: 0.9974 - val_loss: 0.5169 - val_accuracy: 0.9110\n",
      "Epoch 8/10\n",
      "700/700 [==============================] - 5s 7ms/step - loss: 0.0048 - accuracy: 0.9978 - val_loss: 0.5869 - val_accuracy: 0.9101\n",
      "Epoch 9/10\n",
      "700/700 [==============================] - 5s 8ms/step - loss: 0.0060 - accuracy: 0.9977 - val_loss: 0.5938 - val_accuracy: 0.9132\n",
      "Epoch 10/10\n",
      "700/700 [==============================] - 5s 7ms/step - loss: 0.0046 - accuracy: 0.9981 - val_loss: 0.6530 - val_accuracy: 0.9108\n",
      "175/175 [==============================] - 0s 2ms/step - loss: 0.6530 - accuracy: 0.9108\n",
      "Accuracy on test set: 0.9108291864395142\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert TF-IDF sparse matrices to dense numpy arrays\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy on test set: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
    "print(\"Vectorizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jessm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jessm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text and remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "\n",
    "    # Join tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk at mlp_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Define the path where you want to save the model\n",
    "model_path = 'mlp_model.h5'\n",
    "\n",
    "# Save the model\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"Saved model to disk at {model_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EAAEAE5480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "The input text indicates depression.\n",
      "Prediction probability: [0.99999964]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# 2. Define a prediction function\n",
    "def predict_depression(input_text, model_path):\n",
    "    # Preprocess the input text\n",
    "    preprocessed_input = preprocess_text(input_text)\n",
    "    input_vectorized = vectorizer.transform([preprocessed_input]).toarray()\n",
    "\n",
    "    # Load the saved model\n",
    "    loaded_model = load_model(model_path)\n",
    "\n",
    "    # Perform prediction\n",
    "    prediction_prob = loaded_model.predict(input_vectorized)\n",
    "\n",
    "    if prediction_prob[0] >= 0.5:\n",
    "        print(\"The input text indicates depression.\")\n",
    "    else:\n",
    "        print(\"The input text does not indicate depression.\")\n",
    "\n",
    "    return prediction_prob[0]\n",
    "\n",
    "# 3. Get user input and make a prediction\n",
    "model_path = 'mlp_model.h5'\n",
    "user_input = input(\"Enter a text: \")\n",
    "prediction_prob = predict_depression(user_input, model_path)\n",
    "print(f\"Prediction probability: {prediction_prob}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
